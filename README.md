# ğŸ“š PDF RAG Assistant

A local, privacy-focused PDF question-answering system powered by Ollama and RAG (Retrieval-Augmented Generation).

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange.svg)

## âœ¨ Features

- ğŸ”’ **100% Local & Private** - All processing happens on your machine
- ğŸ’° **Completely Free** - No API costs or rate limits
- ğŸš€ **Fast Responses** - Uses efficient embedding + vector search
- ğŸ“„ **PDF Support** - Upload and analyze any PDF document
- ğŸ¤– **Powered by Ollama** - Leverages local LLMs (Llama 3.2)
- ğŸ¨ **Clean UI** - Simple, modern chat interface

## ğŸ› ï¸ Tech Stack

**Backend:**
- FastAPI - REST API framework
- LangChain - RAG orchestration
- FAISS - Vector similarity search
- Ollama - Local LLM inference
- HuggingFace Embeddings - Text vectorization

**Frontend:**
- Vanilla JavaScript
- Tailwind CSS

## ğŸ“‹ Prerequisites

- Python 3.8+
- [Ollama](https://ollama.com/download) installed and running

## ğŸš€ Quick Start

### 1. Clone the repository
```bash
git clone <your-repo-url>
cd pdf-rag-assistant
```

### 2. Install Ollama and pull the model
```bash
# Download from https://ollama.com/download
# Then pull the model:
ollama pull llama3.2:3b
```

### 3. Set up Python environment
```bash
cd backend
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 4. Configure environment variables
Create a `.env` file in the `backend` directory:
```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

### 5. Run the application
```bash
# Start the backend (from backend directory)
uvicorn api:app --reload

# Open in browser
# http://127.0.0.1:8000
```

## ğŸ“– Usage

1. **Upload PDF**: Click "Upload PDF" and select your document
2. **Wait for indexing**: The system will process and vectorize the content
3. **Ask questions**: Type your question and get AI-powered answers based on the document

## ğŸ—ï¸ Project Structure
```
pdf-rag-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api.py              # FastAPI endpoints
â”‚   â”œâ”€â”€ rag_engine/
â”‚   â”‚   â””â”€â”€ __init__.py     # Core RAG logic
â”‚   â”œâ”€â”€ faiss_index/        # Vector store (generated)
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ .env               # Environment config
â””â”€â”€ frontend/
    â””â”€â”€ index.html          # Web interface
```

## ğŸ”§ How It Works

1. **Document Processing**: PDF is extracted and split into chunks
2. **Embedding**: Text chunks are converted to vectors using `all-MiniLM-L6-v2`
3. **Storage**: Vectors are stored in FAISS for fast retrieval
4. **Query**: User question is embedded and similar chunks are retrieved
5. **Generation**: Ollama generates an answer using retrieved context

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open issues or submit pull requests.

## ğŸ“ License

MIT License - feel free to use this project for personal or commercial purposes.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.com) for local LLM inference
- [LangChain](https://langchain.com) for RAG framework
- [FastAPI](https://fastapi.tiangolo.com) for the backend API

---

**Made with â¤ï¸ for privacy-focused AI applications**
